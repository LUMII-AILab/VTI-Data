[{"question":"1. apgalvojums | Lineārās regresijas aplēsēm ir vismazākā dispersija starp visām objektīvajām aplēsēm. 2. apgalvojums| Koeficienti α, kas piešķirti AdaBoost apkopotajiem klasifikatoriem, vienmēr nav negatīvi.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":3},{"question":"1. apgalvojums | RoBERTa pirmsapmācība notiek uz korpusa, kas ir aptuveni 10 reizes lielāks par korpusu, uz kura veikta BERT pirmapmācība. 2. apgalvojums| ResNeXts 2018. gadā parasti izmanto tanh aktivācijas funkcijas.","choices":["True, True","False, nepatiess","True, False","Kļūdaini, Patiesi"],"answer":2},{"question":"Apgalvojums 1| Atbalsta vektoru mašīnas, tāpat kā loģiskās regresijas modeļi, sniedz varbūtības sadalījumu starp iespējamajiem marķējumiem, ņemot vērā ievades piemēru. 2. apgalvojums| Mēs sagaidām, ka atbalsta vektori kopumā paliks nemainīgi, pārejot no lineārā kodola uz augstākas kārtas polinomu kodoliem.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"Mašīnmācīšanās problēma ietver četrus atribūtus un klasi. Katram atribūtam ir 3, 2, 2 un 2 iespējamās vērtības. Klasei ir 3 iespējamās vērtības. Cik maksimāli iespējami dažādi piemēri ir iespējami?","choices":["12","24","48","72"],"answer":3},{"question":"Kura arhitektūra no 2020. gada ir vislabākā augstas izšķirtspējas attēlu klasificēšanai?","choices":["konvolūcijas tīkli","grafu tīkli","pilnībā savienoti tīkli","RBF tīkli"],"answer":0},{"question":"1. apgalvojums | Datu logaritmiskā varbūtība vienmēr palielināsies, veicot secīgas cerību maksimizācijas algoritma iterācijas. 2. apgalvojums| Viens no Qmācīšanās trūkumiem ir tas, ka to var izmantot tikai tad, ja izglītojamajam ir iepriekšējas zināšanas par to, kā viņa darbības ietekmē vidi.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"Pieņemsim, ka esam aprēķinājuši izmaksu funkcijas gradientu un saglabājuši to vektorā g. Kādas ir viena gradientu lejupejoša atjauninājuma izmaksas, ņemot vērā gradientu?","choices":["O(D)","O(N)","O(ND)","O(ND^2)"],"answer":0},{"question":"1. apgalvojums | Nepārtrauktam nejaušam mainīgajam x un tā varbūtības sadalījuma funkcijai p(x) ir spēkā, ka 0 ≤ p(x) ≤ 1 visiem x. 2. apgalvojums| Lēmumu koku apgūst, minimizējot informācijas ieguvumu.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","aplams, patiess"],"answer":1},{"question":"Aplūkojiet turpmāk doto Bejasa tīklu. Cik daudz neatkarīgu parametru ir nepieciešami šim Bejsa tīklam H -> U <- P <- W?","choices":["2","4","8","16"],"answer":2},{"question":"Tā kā apmācības piemēru skaits pieaug līdz bezgalībai, jūsu uz šiem datiem apmācītajam modelim būs:","choices":["mazāka dispersija","augstāka dispersija","Tāda pati dispersija","Neviena no iepriekš minētajām variācijām"],"answer":0},{"question":"1. apgalvojums | Visu taisnstūru kopa 2D plaknē (kas ietver arī taisnstūrus, kas nav piesaistīti asīm) var sašķelt 5 punktu kopu. 2. apgalvojums| K-tuvākā kaimiņa klasifikatora VC-dimensija, ja k = 1, ir bezgalīga.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"_ attiecas uz modeli, kas nespēj ne modelēt mācību datus, ne arī vispārināt jaunus datus.","choices":["laba piemērošanās","pārspīlēta pielāgošana","nepietiekama piemērošana","viss iepriekš minētais"],"answer":2},{"question":"1. apgalvojums | F1 rezultāts var būt īpaši noderīgs datu kopām ar augstu klases nelīdzsvarotību. 2. apgalvojums| Platība zem ROC līknes ir viena no galvenajām metrikām, ko izmanto anomāliju detektoru novērtēšanai.","choices":["patiess, patiess","Kļūdains, Kļūdains","True, False","nepatiess, patiess"],"answer":0},{"question":"1. apgalvojums | Ar atpakaļplūsmas algoritmu tiek apgūts globāli optimāls neironu tīkls ar slēptajiem slāņiem. 2. apgalvojums| Līnijas VC dimensijai jābūt ne lielākai par 2, jo es varu atrast vismaz vienu gadījumu, kad ir 3 punkti, kurus nevar sadrumstalot neviena līnija.","choices":["Taisnība, Taisnība","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"Augsta entropija nozīmē, ka klasifikācijas nodalījumi ir","choices":["tīri","nav tīras","noderīga","bezjēdzīgi"],"answer":1},{"question":"1. apgalvojums | ResNet oriģinālajā dokumentā ir izmantota slāņu normalizācija, nevis sērijveida normalizācija. 2. apgalvojums| DCGAN izmanto pašpievēršanu, lai stabilizētu apmācību.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"Veidojot lineārās regresijas modeli konkrētam datu kopumam, jūs novērojat, ka viena no pazīmēm koeficients ir salīdzinoši liels un negatīvs. Tas liecina, ka","choices":["šim raksturlielumam ir spēcīga ietekme uz modeli (tas būtu jāsaglabā)","Šim raksturlielumam nav spēcīgas ietekmes uz modeli (tas būtu jāignorē).","Nav iespējams komentēt šīs pazīmes nozīmi bez papildu informācijas.","Nav iespējams noteikt neko."],"answer":2},{"question":"Kurš no šiem strukturālajiem pieņēmumiem neironu tīkla gadījumā visvairāk ietekmē kompromisu starp nepietiekamu pielāgošanu (t. i., modeli ar lielu novirzi) un pārmērīgu pielāgošanu (t. i., modeli ar lielu dispersiju):","choices":["Slēpto mezglu skaits","mācīšanās ātrums","sākotnējā svaru izvēle","konstantas terminālvienības ievades izmantošana"],"answer":0},{"question":"Kurš no šiem strukturālajiem pieņēmumiem polinomu regresijai ir tas, kas visvairāk ietekmē kompromisu starp nepietiekamu un pārmērīgu pielāgošanu:","choices":["Polinoma pakāpe","vai mēs mācāmies svarus, izmantojot matricas inversiju vai gradientu nolaišanos","Gausa trokšņa pieņemtā dispersija","konstantas vienības ievades izmantošana"],"answer":0},{"question":"1. apgalvojums | No 2020. gada daži modeļi sasniedz vairāk nekā 98% precizitāti CIFAR-10. 2. apgalvojums| Sākotnējās ResNets netika optimizētas ar Adam optimizatoru.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"K-vidualizācijas algoritms:","choices":["Algoritms: prasa, lai pazīmju telpas dimensija nebūtu lielāka par paraugu skaitu.","mazākā mērķa funkcijas vērtība ir tad, ja K = 1","Minimizē klases iekšējo dispersiju dotajam klasteru skaitam.","Konverģē pie globālā optimuma tikai un vienīgi tad, ja sākotnējie vidējie lielumi ir izvēlēti kā daži no pašiem paraugiem."],"answer":2},{"question":"1. apgalvojums | VGG tīkliem ir konvolūcijas kodoli ar mazāku platumu un augstumu nekā AlexNet pirmā slāņa kodoliem. 2. apgalvojums| No datiem atkarīgas svaru inicializācijas procedūras tika ieviestas pirms sērijveida normalizācijas.","choices":["True, True","False, nepatiess","True, False","aplams, patiess"],"answer":0},{"question":"Kāds ir šādas matricas rangs? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]","choices":["0","1","2","3"],"answer":1},{"question":"1. apgalvojums | Klasifikācijai var izmantot blīvuma aplēsi (piemēram, izmantojot kodola blīvuma aplēsi). 2. apgalvojums| Atbilstība starp loģistisko regresiju un Gausa Naivo Beijes (ar identiskām klašu kovariancēm) nozīmē, ka starp abu klasifikatoru parametriem pastāv vienādība.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":2},{"question":"Pieņemsim, ka mēs vēlamies veikt telpisko datu, piemēram, māju ģeometrisko atrašanās vietu, klasterizāciju. Mēs vēlamies izveidot dažādu izmēru un formu klasterus. Kura no šīm metodēm ir vispiemērotākā?","choices":["Lēmumu koki","Uz blīvumu balstīta klasterizācija","Uz modeļiem balstīta klasterizācija","K-vidējo kopu veidošana"],"answer":1},{"question":"1. apgalvojums | AdaBoost sistēmā kļūdaini klasificēto piemēru svari palielinās ar vienu un to pašu reizinātāju. 2. apgalvojums| AdaBoost sistēmā t-tā vājā klasifikatora mācību kļūdai e_t uz mācību datiem ar svariem D_t ir tendence palielināties kā t funkcija.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"MLE aplēses bieži vien nav vēlamas, jo","choices":["tās ir neobjektīvas","tām ir liela dispersija","tās nav konsekventas aplēses","Neviens no iepriekš minētajiem"],"answer":1},{"question":"Gradienta nolaišanās skaitļošanas sarežģītība ir,","choices":["lineāra D","lineāra N","polinoms D","atkarīga no iterāciju skaita"],"answer":2},{"question":"Vairāku lēmumu koku iznākumu vidējā izteiksmē palīdz _.","choices":["Palielināt novirzi","Samazināt novirzi","Palielināt dispersiju","Samazināt dispersiju"],"answer":3},{"question":"Modelis, kas iegūts, piemērojot lineāro regresiju identificētajai pazīmju apakškopai, var atšķirties no modeļa, kas iegūts apakškopas identificēšanas procesa beigās.","choices":["Labākās apakšgrupas atlases","Pakāpeniskā atlase uz priekšu","Pakāpeniskā atlase","Visi iepriekš minētie"],"answer":2},{"question":"Neironu tīkli:","choices":["Optimizēt izliektu mērķa funkciju","Var apmācīt tikai ar stohastisko gradientu nolaišanos.","Var izmantot dažādu aktivizācijas funkciju kombināciju","Neviens no iepriekš minētajiem"],"answer":2},{"question":"Pieņemsim, ka saslimstība ar slimību D ir aptuveni 5 gadījumi uz 100 cilvēkiem (t. i., P(D) = 0,05). Ļaujiet Bula nejaušajam mainīgajam D nozīmēt, ka pacientam \"ir slimība D\", un ļaujiet Bula nejaušajam mainīgajam TP apzīmēt \"pozitīvi testi\". Zināms, ka slimības D testi ir ļoti precīzi tādā nozīmē, ka pozitīva testa varbūtība, ja jums ir šī slimība, ir 0,99, bet negatīva testa varbūtība, ja jums nav šīs slimības, ir 0,97. Kāda ir P(TP) - iepriekšēja pozitīva testa varbūtība.","choices":["0.0368","0.473","0.078","Neviena no iepriekš minētajām varbūtībām"],"answer":2},{"question":"1 apgalvojums| Pēc attēlošanas pazīmju telpā Q ar radiālās bāzes kodola funkciju 1-NN, izmantojot nesvērtu Eiklīda attālumu, var sasniegt labāku klasifikācijas veiktspēju nekā sākotnējā telpā (lai gan mēs to nevaram garantēt). apgalvojums| Perceptrona VC dimensija ir mazāka nekā vienkārša lineāra SVM VC dimensija.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"Tīkla meklēšanas trūkums ir","choices":["To nevar piemērot nediferencējamām funkcijām.","To nevar piemērot nekontinuālām funkcijām.","To ir grūti īstenot.","tā darbojas samērā lēni daudzkārtējai lineārajai regresijai."],"answer":3},{"question":"Nokrišņu daudzuma prognozēšana reģionā, pamatojoties uz dažādiem rādītājiem, ir ______ problēma.","choices":["Uzraudzītā mācīšanās","Mācīšanās bez uzraudzības","Klasterizēšana","Neviens no iepriekš minētajiem"],"answer":0},{"question":"Kurš no šiem teikumiem ir nepatiess attiecībā uz regresiju?","choices":["Tā saista ievades datus ar izvades datiem.","To izmanto prognozēšanai.","To var izmantot interpretācijai.","Tā atklāj cēloņsakarības"],"answer":3},{"question":"Kurš no šiem iemesliem ir galvenais iemesls lēmumu koka apgriešanai?","choices":["Lai testēšanas laikā ietaupītu skaitļošanas laiku","Lai ietaupītu vietu lēmumu koka glabāšanai","Lai mācību kopas kļūda būtu mazāka","Lai izvairītos no pārāk lielas apmācāmās kopas pielāgošanas"],"answer":3},{"question":"1. apgalvojums | Kodola blīvuma aplēse ir līdzvērtīga kodola regresijas veikšanai ar vērtību Yi = 1\/n katrā sākotnējās datu kopas Xi punktā. 2. apgalvojums| Iemācītā lēmumu koka dziļums var būt lielāks par tā izveidei izmantoto mācību piemēru skaitu.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","aplams, patiess"],"answer":1},{"question":"Pieņemsim, ka jūsu modelis ir pārāk piemērots. Kurš no turpmāk minētajiem NAV pareizs veids, kā mēģināt samazināt pārmērīgu pielāgošanu?","choices":["Palielināt apmācības datu apjomu.","Uzlabot kļūdu minimizēšanai izmantoto optimizācijas algoritmu.","Samazināt modeļa sarežģītību.","Samazināt mācību datu troksni."],"answer":1},{"question":"1. apgalvojums | Softmax funkciju parasti izmanto mutliklases loģistiskajā regresijā. 2. apgalvojums| Neviendabīga softmax sadalījuma temperatūra ietekmē tā entropiju.","choices":["patiess, patiess","False, False","True, False","aplams, patiess"],"answer":0},{"question":"Kurš no šiem apgalvojumiem ir\/ir patiess attiecībā uz SVM?","choices":["Divdimensiju datu punktiem lineārā SVM iemācītā atdalīšanas hiperplāne ir taisna līnija.","Teorētiski Gausa kodola SVM nevar modelēt nevienu sarežģītu atdalīšanas hiperplakni.","Katrai SVM izmantotajai kodola funkcijai var iegūt ekvivalentu bāzes izvērsumu slēgtā formā.","Pārmērīga pielāgošanās SVM nav atbalsta vektoru skaita funkcija."],"answer":0},{"question":"Kura no šādām varbūtībām ir H, U, P un W kopīgā varbūtība, ko apraksta dotais Bejsa tīkls H -> U <- P <- W? [piezīme: kā nosacīto varbūtību reizinājums]","choices":["P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)","Neviens no iepriekš minētajiem"],"answer":2},{"question":"1. apgalvojums | Tā kā SVM ar radiālo bāzes kodolu VC dimensija ir bezgalīga, šādam SVM jābūt sliktākam nekā SVM ar polinomu kodolu, kam ir galīga VC dimensija. apgalvojums| Divslāņu neironu tīkls ar lineārām aktivizācijas funkcijām būtībā ir lineāru separatoru svērta kombinācija, kas apmācīta dotajā datu kopā; uz lineāriem separatoriem balstītais palielināšanas algoritms arī atrod lineāru separatoru kombināciju, tāpēc šie divi algoritmi dos vienādu rezultātu.","choices":["True, True","False, nepatiess","True, False","nepatiess, patiess"],"answer":1},{"question":"1. apgalvojums | ID3 algoritms garantēti atrod optimālo lēmumu koku. 2. apgalvojums| Aplūkojiet nepārtrauktu varbūtības sadalījumu ar blīvumu f(), kas visur nav nulle. Vērtības x varbūtība ir vienāda ar f(x).","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"Ja ir dots neironu tīkls ar N ieejas mezgliem, bez slēptajiem slāņiem, ar vienu izejas mezglu, ar Entropijas zudumu un Sigmoida aktivizācijas funkcijām, kuru no šiem algoritmiem (ar pareiziem hiperparametriem un inicializāciju) var izmantot, lai atrastu globālo optimumu?","choices":["stohastiskais gradientu gradientu nolaišanās (Stochastic Gradient Descent Descent)","Mini-Batch Gradient Descent","Pakāpes gradientu gradientu nolaišanās (Batch Gradient Descent)","Visi iepriekš minētie"],"answer":3},{"question":"Pievienojot vairāk bāzes funkciju lineārajā modelī, izvēlieties visticamāko iespēju:","choices":["Samazina modeļa novirzi","Samazina novērtējuma novirzi","Samazina dispersiju","neietekmē novirzi un dispersiju"],"answer":0},{"question":"Aplūkojiet turpmāk doto Bejasa tīklu. Cik neatkarīgu parametru mums būtu nepieciešams, ja mēs neizdarītu pieņēmumus par neatkarību vai nosacītu neatkarību H -> U <- P <- W?","choices":["3","4","7","15"],"answer":3},{"question":"Cits termins, kas apzīmē ārpus izplatības noteikšanu, ir?","choices":["anomālijas atklāšana","vienas klases noteikšana","apmācības un testa nesakritības robustums","fona noteikšana"],"answer":0},{"question":"1. apgalvojums | Mēs apgūstam klasifikatoru f, pastiprinot vājos izglītojamos h. F lēmuma robežas funkcionālā forma ir tāda pati kā h, bet ar dažādiem parametriem. (piemēram, ja h bija lineārs klasifikators, tad arī f ir lineārs klasifikators). 2. apgalvojums| Krustenisko validāciju var izmantot, lai izvēlētos iterāciju skaitu pastiprinātajā klasifikatorā; šī procedūra var palīdzēt samazināt pārlieku lielu piemērotību.","choices":["True, True","False, nepatiess","True, False","aplams, patiess"],"answer":3},{"question":"1. apgalvojums | Šosejas tīkli tika ieviesti pēc ResNets, un tajos ir atteikšanās no maksimālās apvienošanas par labu konvolūcijām. 2. apgalvojums| Blīvi tīkli parasti izmaksā vairāk atmiņas nekā ResNets.","choices":["Taisnība, Taisnība","aplams, aplams","patiess, aplams","aplams, patiess"],"answer":3},{"question":"Ja N ir gadījumu skaits mācību datu kopā, tad tuvāko kaimiņu klasifikācijas izpildes laiks ir šāds.","choices":["O(1)","O( N )","O(log N )","O( N^2 )"],"answer":1},{"question":"1. apgalvojums | Oriģinālie ResNets un Transformers ir uz priekšu virzīti neironu tīkli. 2. apgalvojums| Sākotnējie Transformers izmanto pašpievēršanu, bet sākotnējie ResNet to neizmanto.","choices":["patiess, patiess","Kļūdains, Kļūdains","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"1. apgalvojums | RELU nav monotoni, bet sigmoīdi ir monotoni. 2. apgalvojums| Neironu tīkli, kas apmācīti ar gradienta nolaišanās metodi, ar lielu varbūtību konverģē pie globālā optimuma.","choices":["patiess, patiess","Maldīgs, Maldīgs","patiess, aplams","nepatiess, patiess"],"answer":3},{"question":"Nervu tīkla sigmoidālā mezgla skaitliskais izvads:","choices":["ir neierobežots, ietver visus reālos skaitļus.","Ir neierobežots, ietver visus veselos skaitļus.","ir ierobežots starp 0 un 1.","Ir ierobežots no -1 līdz 1."],"answer":2},{"question":"Kuru no šīm metodēm var izmantot tikai tad, ja mācību dati ir lineāri atdalāmi?","choices":["Lineārs SVM ar cieto robežu.","Lineārā loģistiskā regresija.","Lineārā SVM ar mīksto rezervi.","Centroidu metode."],"answer":0},{"question":"Kurš no šiem ir telpiskās klasterizācijas algoritms?","choices":["Uz sadalīšanu balstīta klasterizācija","K-izmēra klasterizācija","Uz režģi balstīta klasterizācija","Visi iepriekš minētie"],"answer":3},{"question":"1. apgalvojums | Atbalsta vektoru mašīnu konstruētajām maksimālās rezerves lēmumu robežām ir viszemākā vispārināšanas kļūda starp visiem lineārajiem klasifikatoriem. 2. apgalvojums| Jebkuru lēmuma robežu, ko mēs iegūstam no ģeneratīvā modeļa ar klases nosacījuma Gausa sadalījumu, principā var atveidot ar SVM un polinomu kodolu, kura pakāpe ir mazāka vai vienāda ar trīs.","choices":["patiess, patiess","aplams, aplams","True, False","aplams, patiess"],"answer":3},{"question":"1. apgalvojums | Lineāro modeļu L2 regularizācijai ir tendence padarīt modeļus retākus nekā L1 regularizācijai. 2. apgalvojums| Atlikušos savienojumus var atrast Resnet un Transformers.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":3},{"question":"Pieņemsim, ka mēs vēlamies aprēķināt P(H|E, F) un mums nav nosacītās neatkarības informācijas. Kura no šādām skaitļu kopām ir pietiekama aprēķinam?","choices":["P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)"],"answer":1},{"question":"Kurš no turpmāk minētajiem faktoriem novērš pārmērīgu pielāgošanu, kad mēs veicam bagging?","choices":["Paraugu ņemšana ar nomaiņu kā paraugu ņemšanas metode","vāju klasifikatoru izmantošana","Klasifikācijas algoritmu izmantošana, kas nav pakļauti pārmērīgai piemērošanai.","Validācijas prakse, ko veic katram apmācītajam klasifikatoram"],"answer":1},{"question":"1. apgalvojums | PCA un spektrālā klasterizācija (piemēram, Endrjū Ng (Andrew Ng)) veic eigendekompozīciju divām dažādām matricām. Tomēr šo divu matricu izmēri ir vienādi. apgalvojums| Tā kā klasifikācija ir regresijas īpašs gadījums, tad loģistiskā regresija ir lineārās regresijas īpašs gadījums.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"1. apgalvojums | Stenfordas sentimentu krātuvē bija iekļautas filmu, nevis grāmatu recenzijas. 2. apgalvojums| Penn Treebank tika izmantota valodas modelēšanai.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"Kāda ir šādas matricas nulles telpas dimensija? A = [[3, 2, -9], [-6, -4, 18], [12, 8, -36]]","choices":["0","1","2","3"],"answer":2},{"question":"Kas ir atbalsta vektori?","choices":["Piemēri, kas atrodas vistālāk no lēmuma robežas.","Vienīgie piemēri, kas nepieciešami, lai aprēķinātu f(x) SVM.","Datu centrodi.","Visi piemēri, kuriem SVM ir nenulles svars αk."],"answer":1},{"question":"Paziņojums 1| Word2Vec parametri netika inicializēti, izmantojot ierobežotu Boltzman Machine. 2. apgalvojums| Tanh funkcija ir nelineāra aktivācijas funkcija.","choices":["True, True","False, aplams, aplams","True, False","False, True"],"answer":0},{"question":"Ja mācību zaudējumi palielinās, pieaugot epohu skaitam, kura no šādām iespējamām problēmām varētu būt saistīta ar mācīšanās procesu?","choices":["Regularizācija ir pārāk zema, un modelis ir pārlieku pielāgots","Regularizācija ir pārāk augsta, un modelis ir nepietiekami piemērots.","soļa lielums ir pārāk liels","Soļa lielums ir pārāk mazs"],"answer":2},{"question":"Pieņemsim, ka saslimstība ar slimību D ir aptuveni 5 gadījumi uz 100 cilvēkiem (t. i., P(D) = 0,05). Ļaujiet Bula nejaušajam mainīgajam D nozīmēt, ka pacientam \"ir slimība D\", un ļaujiet Bula nejaušajam mainīgajam TP apzīmēt \"pozitīvi testi\". Zināms, ka slimības D testi ir ļoti precīzi tādā nozīmē, ka pozitīva testa varbūtība, ja jums ir šī slimība, ir 0,99, bet negatīva testa varbūtība, ja jums nav šīs slimības, ir 0,97. Kāda ir P(D | TP), pēcpārbaudes varbūtība, ka jums ir slimība D, ja tests ir pozitīvs?","choices":["0.0495","0.078","0.635","0.97"],"answer":2},{"question":"1. apgalvojums | Tradicionālie mašīnmācīšanās rezultāti pieņem, ka apmācāmā un testa kopas ir neatkarīgas un identiski sadalītas. 2. apgalvojums| 2017. gadā COCO modeļi parasti tika iepriekš apmācīti ImageNet.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"1. apgalvojums | Divu dažādu kodolu K1(x, x0) un K2(x, x0) iegūtās robežas vērtības vienai un tai pašai apmācības kopai nenorāda, kurš no klasifikatoriem uzrādīs labākus rezultātus testa kopā. apgalvojums| BERT aktivācijas funkcija ir GELU.","choices":["patiess, patiess","False, False","patiess, aplams","nepatiesa, patiesa"],"answer":0},{"question":"Kurš no šiem algoritmiem ir mašīnmācīšanās klasterizācijas algoritms?","choices":["Gaidījumu maksimizācija","CART","Gausa Naivās Beijesas (Gaussian Naïve Bayes)","Apriori"],"answer":0},{"question":"Jūs tikko esat pabeidzis apmācīt lēmumu koku surogātpasta klasifikācijai, un tā veiktspēja gan mācību, gan testa kopās ir ārkārtīgi slikta. Jūs zināt, ka jūsu implementācijā nav kļūdu, tad kas varētu būt problēmas cēlonis?","choices":["Jūsu lēmumu koki ir pārāk sekli.","Jums ir jāpalielina mācīšanās ātrums.","Jūs esat pārlieku pielāgojies.","Neviens no iepriekš minētajiem."],"answer":0},{"question":"K-kārtēja krusteniskā validācija ir","choices":["lineāra attiecībā pret K","kvadrātiska attiecībā pret K","kubiska attiecībā pret K","eksponenciāla K"],"answer":0},{"question":"1. apgalvojums | Rūpnieciska mēroga neironu tīkli parasti tiek apmācīti ar CPU, nevis GPU. 2. apgalvojums| ResNet-50 modelim ir vairāk nekā 1 miljards parametru.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","Kļūdains, Patiess"],"answer":1},{"question":"Ja ir doti divi Bula nejaušie lielumi A un B, kur P(A) = 1\/2, P(B) = 1\/3 un P(A | ¬B) = 1\/4, kāds ir P(A | B)?","choices":["1\/6","1\/4","3\/4","1"],"answer":3},{"question":"Ar kuru no turpmāk minētajiem profesoriem visbiežāk ir saistīti mākslīgā intelekta radītie eksistenciālie riski?","choices":["Nando de Frietas","Yann LeCun","Stjuartu Raselu","Jitendra Malik"],"answer":2},{"question":"1. apgalvojums | Maksimizējot loģistiskās regresijas modeļa varbūtību, iegūst vairākus lokālos optimumus. 2. apgalvojums| Neviens klasifikators nevar darboties labāk par naivā Bayes klasifikatoru, ja ir zināms datu sadalījums.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","Kļūdains, Patiess"],"answer":1},{"question":"Kurš no šiem strukturālajiem pieņēmumiem kodolu regresijas gadījumā visvairāk ietekmē kompromisu starp nepietiekamu un pārmērīgu pielāgošanu:","choices":["Vai kodola funkcija ir Gausa vai trīsstūrveida vai kastes formas?","vai mēs izmantojam Eiklīda vai L1 vai L∞ metriku.","kodola platums","Kodola funkcijas maksimālais augstums"],"answer":2},{"question":"1. apgalvojums | SVM mācīšanās algoritms garantēti atrod globāli optimālu hipotēzi attiecībā uz tā objekta funkciju. 2. apgalvojums| Pēc kartēšanas pazīmju telpā Q, izmantojot radiālā pamata kodola funkciju, perceptrons var sasniegt labāku klasifikācijas veiktspēju nekā savā sākotnējā telpā (lai gan mēs to nevaram garantēt).","choices":["patiess, patiess","aplams, aplams","True, False","aplams, patiess"],"answer":0},{"question":"Kurš no šiem strukturālajiem pieņēmumiem ir tas, kas visvairāk ietekmē kompromisu starp nepietiekamu un pārmērīgu pielāgošanu Gausa Bejas klasifikatoram:","choices":["Vai mēs mācāmies klases centrus, izmantojot maksimālo ticamību vai gradienta gradientu (Gradient Descent)?","vai pieņemam pilnas klases kovariāciju matricas vai diagonālas klases kovariāciju matricas","vai mums ir vienādas klases priori vai no datiem novērtēti priori.","Vai mēs pieļaujam, ka klasēm ir atšķirīgi vidējie vektori, vai piespiežam tās dalīt vienu un to pašu vidējo vektoru."],"answer":1},{"question":"1. apgalvojums | Pārmērīga piemērošana ir iespējamāka, ja mācību datu kopa ir maza. 2. apgalvojums| Pārmērīga piemērošana ir ticamāka, ja hipotēžu telpa ir maza.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":3},{"question":"Apgalvojums 1| Papildus EM, gradientu nolaišanos var izmantot, lai veiktu secinājumus vai mācīšanos par Gausa maisījuma modeli. 2. apgalvojums | Pieņemot fiksētu atribūtu skaitu, uz Gausa-Beisa balstītu optimālu klasifikatoru var apgūt laikā, kas ir lineārs ierakstu skaitam datu kopā.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"1. apgalvojums | Bayesian tīklā savienojuma koka algoritma secinājumu rezultāti ir tādi paši kā mainīgo izslēgšanas secinājumu rezultāti. 2. apgalvojums| Ja divi nejauši mainīgie X un Y ir nosacīti neatkarīgi, ņemot vērā citu nejauši mainīgo Z, tad atbilstošajā Bejasa tīklā X un Y mezgli ir d-atdalīti, ņemot vērā Z.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":2},{"question":"Ņemot vērā lielu datu kopu ar sirds slimību pacientu medicīniskajiem ierakstiem, mēģiniet noskaidrot, vai varētu būt dažādas šādu pacientu grupas, kurām varētu piemērot atsevišķu ārstēšanu. Kāda veida mācīšanās problēma ir šī?","choices":["Uzraudzīta mācīšanās","Mācīšanās bez uzraudzības","Gan a), gan b)","Ne (a), ne (b)"],"answer":1},{"question":"Ko jūs darītu PCA, lai iegūtu tādu pašu projekciju kā SVD?","choices":["Pārveidot datus uz nulles vidējo vērtību","Pārveidotu datus uz nulles mediānu","Nav iespējams","Neviens no šiem variantiem"],"answer":0},{"question":"1. apgalvojums | 1 tuvākā kaimiņa klasifikatora apmācības kļūda ir 0. 2. apgalvojums| Datu punktu skaitam pieaugot līdz bezgalībai, MAP aplēse tuvojas MLE aplēsei visām iespējamām prioritātēm. Citiem vārdiem sakot, ja ir pietiekami daudz datu, prioritātes izvēlei nav nozīmes.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","aplams, patiess"],"answer":2},{"question":"Veicot mazāko kvadrātu regresiju ar regulēšanu (pieņemot, ka optimizāciju var veikt precīzi), palielinot regulēšanas parametra λ vērtību, palielinās testēšanas kļūda.","choices":["nekad nesamazinās apmācības kļūdu.","nekad nepalielinās apmācības kļūdu.","nekad nesamazinās testēšanas kļūdu.","nekad nepalielinās"],"answer":0},{"question":"Kurš no šiem aprakstiem vislabāk raksturo to, ko diskriminatīvās pieejas mēģina modelēt? (w ir modeļa parametri)","choices":["p(y|x, w)","p(y, x)","p(w|x, w)","Neviens no iepriekš minētajiem"],"answer":0},{"question":"1. apgalvojums | CIFAR-10 konvolūcijas neironu tīklu klasifikācijas veiktspēja var pārsniegt 95 %. 2. apgalvojums| Neironu tīklu kopas neuzlabo klasifikācijas precizitāti, jo to apgūtās atveides ir ļoti korelēti saistītas.","choices":["patiess, patiess","Kļūdains, Kļūdains","patiess, aplams","nepatiess, patiess"],"answer":2},{"question":"Par kuru no šiem punktiem Bejsī un frequentisti varētu būt atšķirīgi viedokļi?","choices":["Ne-Gausa trokšņa modeļa izmantošana varbūtiskajā regresijā.","Varbūtības modeļa izmantošana regresijā.","Prioritāro sadalījumu izmantošana parametriem varbūtības modelī.","Klases prioritāšu izmantošana Gausa diskriminantu analīzē."],"answer":2},{"question":"1. apgalvojums | BLEU metrika izmanto precizitāti, bet ROGUE metrika izmanto atsaukumu. 2. apgalvojums| Angļu valodas teikumu modelēšanai bieži tika izmantoti slēptie Markova modeļi.","choices":["Taisnība, Taisnība","Kļūdaini, Kļūdaini","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"1. apgalvojums | ImageNet ir attēli ar dažādu izšķirtspēju. 2. apgalvojums| Caltech-101 ir vairāk attēlu nekā ImageNet.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":2},{"question":"Kurš no šiem variantiem ir piemērotāks, lai veiktu pazīmju atlasi?","choices":["Ridge","Lasso","gan a), gan b)","ne (a), ne (b)"],"answer":1},{"question":"Pieņemsim, ka jums ir dots EM algoritms, kas atrod maksimālās ticamības aplēses modelim ar latentajiem mainīgajiem. Jums tiek lūgts modificēt algoritmu tā, lai tas tā vietā atrastu MAP aplēses. Kurš solis vai soļi jums jāmaina?","choices":["Sagaidījums","Maksimizācija","Nav nepieciešamas nekādas izmaiņas","Abi"],"answer":1},{"question":"Kurš no šiem strukturālajiem pieņēmumiem ir tas, kas visvairāk ietekmē kompromisu starp nepietiekamu un pārmērīgu pielāgošanu Gausa Bejas klasifikatoram:","choices":["Vai mēs mācāmies klases centrus, izmantojot maksimālo ticamību vai gradienta gradientu (Gradient Descent)?","vai pieņemam pilnas klases kovariāciju matricas vai diagonālas klases kovariāciju matricas","vai mums ir vienādas klases priori vai no datiem novērtēti priori","vai mēs pieļaujam, ka klasēm ir atšķirīgi vidējie vektori, vai piespiežam tās dalīt vienu un to pašu vidējo vektoru"],"answer":1},{"question":"1. apgalvojums | Jebkuriem diviem mainīgajiem x un y ar kopīgu sadalījumu p(x, y) vienmēr ir H[x, y] ≥ H[x] + H[y], kur H ir entropijas funkcija. 2. apgalvojums| Dažiem virzītajiem grafiem moralizācija samazina grafā esošo malu skaitu.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":1},{"question":"Kurš no šiem aspektiem NAV uzraudzīta mācīšanās?","choices":["PCA","Lēmumu koks","Lineārā regresija","Naivā Bejsija"],"answer":0},{"question":"1. apgalvojums | Neironu tīkla konverģence ir atkarīga no mācīšanās ātruma. 2. apgalvojums| Izslēgšana reizina nejauši izvēlētās aktivācijas vērtības ar nulli.","choices":["patiess, patiess","False, False","patiess, aplams","aplams, patiess"],"answer":0},{"question":"Kurš no šiem gadījumiem ir vienāds ar P(A, B, C), ja ir doti Bula nejaušie lielumi A, B un C un starp nevienu no tiem nav neatkarības vai nosacītas neatkarības pieņēmumu?","choices":["P(A | B) * P(B | C) * P(C | A)","P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)"],"answer":2},{"question":"Kuru no šiem uzdevumiem vislabāk var atrisināt, izmantojot klasterizāciju.","choices":["Nokrišņu daudzuma prognozēšana, pamatojoties uz dažādiem rādītājiem","Krāpniecisku kredītkaršu darījumu atklāšana","Apmācīt robotu atrisināt labirintu","Visi iepriekš minētie"],"answer":1},{"question":"Pēc regulēšanas soda piemērošanas lineārajā regresijā jūs konstatējat, ka daži w koeficienti ir nulle. Kurš no šiem sodiem varētu būt ticis izmantots?","choices":["L0 norma","L1 norma","L2 norma","a) vai b)"],"answer":3},{"question":"A un B ir divi notikumi. Ja P(A, B) samazinās, bet P(A) palielinās, kurš no šiem apgalvojumiem ir patiess?","choices":["P(A|B) samazinās","P(B|A) samazinās","P(B) samazinās","Viss iepriekš minētais"],"answer":1},{"question":"1. apgalvojums | Mācoties HMM fiksētam novērojumu kopumam, pieņemot, ka mēs nezinām patieso slēpto stāvokļu skaitu (kas bieži notiek), mēs vienmēr varam palielināt mācību datu varbūtību, atļaujot vairāk slēpto stāvokļu. 2. apgalvojums| Sadarbības filtrēšana bieži vien ir noderīgs modelis, lai modelētu lietotāju izvēli attiecībā uz filmām.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"Jūs apmācāt lineārās regresijas modeli vienkāršam aplēšu uzdevumam un pamanāt, ka modelis pārāk lielā mērā pielāgojas datiem. Jūs nolemjat pievienot $\\ell_2$ regularizāciju, lai sodītu svaru. Palielinot $\\ell_2$ regulēšanas koeficientu, kas notiks ar modeļa novirzi un dispersiju?","choices":["Novirze palielināsies ; Variance palielināsies","Novirzes palielināšanās ; dispersijas samazināšanās","Novirze samazināsies ; Variance palielināsies","Novirzes samazināšanās ; Variances samazināšanās"],"answer":1},{"question":"Kura PyTorch 1.8 komanda(-as) rada $10\\reiz 5$ Gausa matricu ar katru ierakstu i.i.d., kas atlasīts no $\\mathcal{N}(\\mu=5,\\sigma^2=16)$ un $10\\reiz 10$ viendabīgu matricu ar katru ierakstu i.i.d., kas atlasīts no $U[-1,1)$?","choices":["\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{torch.rand(10,10,low=-1,high=1)} }","\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{(torch.rand(10,10) - 0,5) \/ 0,5}}","\\texttt{5 + torch.randn(10,5) * 4} ; \\texttt{2 * torch.rand(10,10) - 1}","\\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \\texttt{2 * torch.rand(10,10) - 1}}."],"answer":2},{"question":"1. apgalvojums | ReLU gradients ir nulle, ja $x<0$, un sigmoidālais gradients $\\sigma(x)(1-\\sigma(x))\\le \\frac{1}{4}$ visiem $x$. 2. apgalvojums| Sigmoidai ir nepārtraukts gradients, bet ReLU ir pārtraukts gradients.","choices":["patiess, patiess","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":0},{"question":"Kas ir taisnība par partijas normalizēšanu?","choices":["Pēc partijas normalizēšanas slāņa aktivācijas atbilst standarta Gausa sadalījumam.","Afīno slāņu novirzes parametrs kļūst lieks, ja uzreiz pēc tam seko sērijveida normalizācijas slānis.","Lietojot sērijveida normalizāciju, ir jāmaina standarta svaru inicializācija.","Partiju normalizācija ir līdzvērtīga konvolūcijas neironu tīklu slāņu normalizācijai."],"answer":1},{"question":"Pieņemsim, ka mums ir šāda mērķa funkcija: $\\argmin_{w} \\frac{1}{2} \\norm{Xw-y}^2_2 + \\frac{1}{2}\\gamma \\norm{w}^2_2$ Kāds ir $\\frac{1}{2} gradients? \\norm{Xw-y}^2_2 + \\frac{1}{2}\\lambda \\norm{w}^2_2$ attiecībā pret $w$?","choices":["$\\nabla_w f(w) = (X^\\top X + \\lambda I)w - X^\\top y + \\lambda w$.","$\\nabla_w f(w) = X^\\top X w - X^\\top y + \\lambda$","$\\nabla_w f(w) = X^\\top X w - X^\\top y + \\lambda w$","$\\nabla_w f(w) = X^\\top X w - X^\\top y + (\\lambda+1) w$"],"answer":2},{"question":"Kurš no šiem apgalvojumiem attiecas uz konvolūcijas kodolu?","choices":["Konvertējot attēlu ar $\\\\begin{bmatrix}1 & 0 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$, attēls nemainīsies.","Attēla konvolvēšana ar $\\begin{bmatrix}0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$ nemainītu attēlu.","Attēla konvolvēšana ar $\\begin{bmatrix}1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\end{bmatrix}$ nemainītu attēlu.","Attēla konvolvēšana ar $\\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$ nemainītu attēlu."],"answer":1},{"question":"Kurš no šiem apgalvojumiem ir nepatiess?","choices":["Semantiskās segmentācijas modeļi prognozē katra pikseļa klasi, bet daudzklases attēlu klasifikatori prognozē visa attēla klasi.","Robežkoks ar IoU (intersection over union), kas ir vienāds ar $96\\%$, visticamāk, tiks uzskatīts par patiesi pozitīvu.","Ja prognozētais robežlaukums neatbilst nevienam objektam ainā, tas tiek uzskatīts par viltus pozitīvu.","Robežlaukums, kura IoU (intersection over union) ir vienāds ar $3\\%$, visticamāk, tiks uzskatīts par kļūdaini negatīvu."],"answer":3},{"question":"Kurš no šiem apgalvojumiem ir nepatiess?","choices":["Šāds pilnībā savienots tīkls bez aktivācijas funkcijām ir lineārs: $g_3(g_2(g_1(x)))$, kur $g_i(x) = W_i x$ un $W_i$ ir matricas.","Leaky ReLU $\\max\\{0,01x,x\\}$ ir izliekta.","ReLU kombinācija, piemēram, $ReLU(x) - ReLU(x-1)$, ir izliekta.","Zaudējums $\\log \\sigma(x)= -\\log(1+e^{-x})$ ir ieliekts."],"answer":2},{"question":"Mēs apmācām pilnībā savienotu tīklu ar diviem slēptajiem slāņiem, lai prognozētu mājokļu cenas. Ievaddati ir $100$-dimensiju, un tiem ir vairāki raksturlielumi, piemēram, kvadrātpēdu skaits, vidējie ģimenes ienākumi utt. Pirmajam slēptajam slānim ir $1000$ aktivācijas. Otrā slēptā slāņa aktivācijas ir $10$. Izvads ir skalārs, kas attēlo mājas cenu. Pieņemot, ka šis tīkls ir vaniļas tīkls ar afinām transformācijām, bez partijas normalizācijas un bez mācāmiem parametriem aktivācijas funkcijā, cik parametru ir šim tīklam?","choices":["111021","110010","111110","110011"],"answer":0},{"question":"1. apgalvojums | Sigmoīda $\\sigma(x)=(1+e^{-x})^{-1}$ atvasinājums attiecībā pret $x$ ir vienāds ar $\\text{Var}(B)$, kur $B\\sim \\text{Bern}(\\sigma(x))$ ir Bernuļa nejaušais lielums. apgalvojums| Novirzes parametru iestatīšana katrā neironu tīkla slānī uz 0 maina novirzes un variances kompromisu tā, ka modeļa dispersija palielinās un modeļa novirze samazinās.","choices":["True, True","aplams, aplams","patiess, aplams","nepatiess, patiess"],"answer":2}]